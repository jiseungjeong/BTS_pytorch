{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ReLU와 ReLU의 변형들\n",
    "기울기 소실을 완화하는 가장 간단한 방법, 은닉층의 활성화 함수로 시그모이드나 하이퍼 볼릭 탄젠트 함수 대신에 ReLU나 Leaky ReLU와 같은 ReLU의 변형 함수를 사용하는 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 가중치 초기화 (Weight initialization)\n",
    "가중치 초기화를 적절히 하면 기울기 소실 문제를 해결할 수도 있다. 가중치의 초기값에 따라 훈련 양상이 바뀐다.\n",
    "### 1. 세이비어 초기화 (Xavier Initialization, 글로럿 초기화, Glorot)\n",
    "유니폼 분포와 정규 분포로 초기화 하는 경우, 2가지로 나뉨.\n",
    "여러층의 기울기 분산 사이에 균형을 반춰서 특정 층이 너무 주목을 받거나 다른층이 뒤쳐지는 것을 막음.\n",
    "유선형 은닉층(시그모이드, 하이퍼볼릭 탄젠트)과 사용했을 때 좋은 성능을 보이지만, 직선형(ReLU/변형 렐루) 은닉층과 사용하면 성능 안 좋음.\n",
    "직선형 은닉층에서는 **He 초기화(He initialization)** 사용함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 배치 정규화 (Batch Normalization)\n",
    "인공 신경망 각 층에 들어가는 입력을 평균과 분상으로 정규화 하여 학습을 효율적으로 만듦.\n",
    "### 1. 내부 공변량 변화(Internal Covariate Shift)\n",
    "학습 과정에서 **층 별로 데이터 분포가 달라지는 현상.**\n",
    "이전 층의 학습에 의해 이전 층의 가중치 값이 바뀌게 되면, 현재 층에 전달되는 입력 데이터의 분포가 현재층이 학습했던 시점의 분포와 차이가 발생함.\n",
    "배치 정규화를 제안한 논문에서는 기울기 소실/폭주 등의 딥러닝 모델의 불안정성이 층마다 입력의 분포가 달라지기 때문이라고 주장함.\n",
    "\n",
    "- 공변량 변화는 **훈련 데이터의 분포와 테스트 데이터의 분포가 다른** 경우를 의미함.\n",
    "- 내부 공변량 변화는 **신경망 층 사이**에서 발생하는 입력 데이터의 **분포 변화**를 의미함.\n",
    "\n",
    "### 2. 배치 정규화 (Batch Normalization)\n",
    "배치 단위로 정규화.\n",
    "각층에서 활성화 함수 통과 전에 수행됨.\n",
    "입력에 대해 평균을 0으로 만들고 정규화를 함. 그리고 정규화된 데이터에 대해 스케일과 시프트를 수행함.\n",
    "- 배치 정규화를 사용하면 유선형 함수를 사용하더라도 기울기 소실 문제가 크게 개선됨.\n",
    "- 훨씬 큰 학습률을 사용할 수 있음. 그래서 학습 속도 빠르게 할 수 있음.\n",
    "- 가중치 초기화에 덜 민감해짐\n",
    "- 미니 배치마다 평균과 표준편차를 계산하므로 훈련 데이터에 일종의 잡음을 넣는 부수 효과로 과적합을 방지하는 효과도 낸다. 하지만 드롭아웃을 사용하는 것이 과적합에 더 효과적이다.\n",
    "- 배치 정규화는 모델을 복잡하게 하며, 추가 계산을 하는 것이라 테스트 데이터에 대한 예측 시 실행 시간이 느려짐. 서비스 속도 관점에서 배치 정규화는 부정적임. \n",
    "- 배치 정규화의 효과는 굉장하지만 내부 공변량 때문에 기울기 소실/폭주가 일어나는 것이 아니라는 논문도 있음.\n",
    "\n",
    "### 3. 배치 정규화의 한계\n",
    "배치 정규화는 뛰어난 방법이지만 한계가 있음.\n",
    "##### 1. 미니배치 크기에 의존적이다.\n",
    "너무 작은 크기의 배치에서는 배치 정규화 잘 작동하지 않음. 작은 미니 배치에서는 배치 정규화의 효과가 극단적으로 작용되어 훈련에 악영향을 줄 수 있음. 배치 크기가 어느정도 클 때 배치 정규화를 사용해야함.\n",
    "##### 2. RNN에 적용하기 어렵다.\n",
    "RNN은 각 시점마다 다른 통계치를 가짐. 이는 배치 정규화를 적용시키기 어렵게 만듦.\n",
    "**층 정규화는 배치 크기에 의존적이지 않으며, RNN에 수월히 적용됨.**\n",
    "\n",
    "### 4. 층 정규화 (Layer Normalization)\n",
    "배치 정규화는 한 피쳐에서 배치들을 정규화시키는(a,b,c의 키) 반면, 층 정규화는 여러 피쳐의 각 배치 엘리먼트를 정규화(a의 키, 몸무게, 나이)한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
